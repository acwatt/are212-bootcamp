{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07a62193",
   "metadata": {},
   "source": [
    "# Python bootcamp\n",
    "\n",
    "This bootcamp will be teaching python through a just-in-time programming approach. That is, instead of walking you through fundamentals step by step, we will jump right into analyzing code and teaching you the tools necessary to understand and modify that code as we go. You can find a more systematic review of python fundamentals in the subfolders of this repository, with the following organization:\n",
    "- ``0_getting_started`` contains information on what python is and its current popularity and use.\n",
    "- ``1_fundamentals`` gives an intro to datatypes, the building blocks of any programming language.\n",
    "- ``2_functions`` introduces you to functions and scope in python.\n",
    "- ``3_classes`` gives a basic introduction to python classes and a more advanced example of an OLS class.\n",
    "- ``5_list_comprehensions`` introduces list comprehensions, an efficient method of creating iterable objects.\n",
    "\n",
    "This notebook starts with an abitious goal: the creation of a python ``class`` object (don't worry, we'll tell you what this is) that runs OLS on data. To do this, we will introduce concepts like importing packages, defining functions, and manipulating numpy matrices, then organize all of these into a `class` object.  Specifically our goals are going to be:\n",
    "- Make a linear projection function with b0, b1, x as input, y as output\n",
    "- Write a data generating function\n",
    "- Give a brief explanation of the scipy.optimize.minimize function\n",
    "- Minimize the squared errors to estimate b0 and b1\n",
    "- Create a class that implements the same minimization, \n",
    "  that takes data in instantiation, and has an 'estimate' method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e4c62",
   "metadata": {},
   "source": [
    "## Package imports\n",
    "\n",
    "We will be using object types and methods from a couple of different packages in python. These packages must first be installed in the environment you are working in. For the datahub environment we are using for this bootcamp, the necessary packages are already installed. If you want to work on your own computer, you will need to install these using either ``pip``, python's native installer, or preferably the package manager Anaconda. Lucy will go over installation using Anaconda on Friday. For now, note that python packages are importing using the ``import`` command, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "13acb569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for later\n",
    "import numpy as np\n",
    "from scipy.stats import distributions as iid\n",
    "from scipy.stats import rv_continuous"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6eba71a",
   "metadata": {},
   "source": [
    "## Functions: Building blocks of an OLS class\n",
    "We are going to start by writing functions for the main actions of an OLS class and for generating the simmulated data we will use to test our OLS functions. An OLS estimator must do two things; define a linear model and (to start) minimize the sum of squared errors. We can summarize these activities as:\n",
    "\n",
    "1. Linear projection: Predict \"y\" given a set of betas and data X\n",
    "    - inputs: b0, b1, x\n",
    "    - outputs: y hat\n",
    "2. Define the data\n",
    "    - Inputs: N, true betas\n",
    "    - Outputs: y, X matrices\n",
    "3. Minimizer function: minimizes the squared distance between the linear projection and y\n",
    "    - inputs: A function to minimize (SSE)\n",
    "    - output: betas that minimize that function\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83387980",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afde2e22",
   "metadata": {},
   "source": [
    "### 1. Linear projection\n",
    "Suppose we have a vector X that is N by 2, where the first column is a column of ones, and a vector of betas: b = [b0, b1]. The projection matrix, or the matrix that predicts y, is given by $Xb$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "698efc88",
   "metadata": {},
   "source": [
    "#### Aside 1: Matrix algebra and some helpful numpy functions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9396d47",
   "metadata": {},
   "source": [
    "``numpy`` is a python package that contains number generators, its own matrix objects and methods, and more! Plus it's blazing fast. The building blocks of numpy are called numpy arrays, and they can have as many dimentions as you want. The next block of code shows you how to create some numpy arrays.\n",
    "\n",
    "Note that we have renamed `numpy` to `np` when importing into this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "fbdee978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My matrix shapes are\n",
      " m1: (3,)\n",
      " m2: (1, 3)\n",
      " m3: (2, 3)\n",
      " m4: (3, 1)\n"
     ]
    }
   ],
   "source": [
    "m1 = np.array([1, 2, 3]) # shape (3,)\n",
    "m2 = np.array([[1, 2, 3]]) # shape (1,3)\n",
    "m3 = np.array([[1, 2, 3], # shape (2,3)\n",
    "               [4, 5, 6]])\n",
    "m4 = np.array([[2], [2], [2]]) # shape (3,1)\n",
    "# We can inspect an array's shape\n",
    "print(f\"My matrix shapes are\\n m1: {m1.shape}\\n m2: {m2.shape}\\n m3: {m3.shape}\\n m4: {m4.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9fe1def7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplying m3, 4 gives:\n",
      " [[12]\n",
      " [30]]\n",
      "Element-wise multiplying m1, m3 gives:\n",
      " [[1 4 9]]\n"
     ]
    }
   ],
   "source": [
    "# numpy arrays can be added with +, matrix multiplied with @, or element-wise multiplied with *:\n",
    "matmult = m3@m4\n",
    "elementmult = m2*m1\n",
    "print(f\"Matrix multiplying m3, 4 gives:\\n {matmult}\")\n",
    "print(f\"Element-wise multiplying m1, m3 gives:\\n {elementmult}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1828c34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 9]\n",
      "[ 4  8 12]\n"
     ]
    }
   ],
   "source": [
    "# they can also be (element-wise) raised to powers, etc:\n",
    "print(m1**2)\n",
    "print(m1*4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0517e34",
   "metadata": {},
   "source": [
    "*end of Aside 1* \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f46a9ce4",
   "metadata": {},
   "source": [
    "#### Back to the linear projection function..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80f3a8c1",
   "metadata": {},
   "source": [
    "Note that the <font color='#ba2121'>text in red</font> documents the function, telling future users (and your future self!) what arguments the function takes in, and what it returns. This \"docstring\" is optional, but good practice. Tip: being detailed about what data types are acceptable will help you even more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9b62dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_projection(X, b):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - X: numpy array of dimensions NxK. The first column is assumed to be a column of ones.\n",
    "        - b: numpy array of dimensions Kx1.\n",
    "    Returns:\n",
    "        - Xb, an Nx1 numpy array\n",
    "    '''\n",
    "    # make sure b is the right shape\n",
    "    b = b.reshape((len(b),1))\n",
    "    return X@b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "016595a1",
   "metadata": {},
   "source": [
    "We can test this with random X and b values. In python, we can use `numpy.random.rand(a,b)` to sample numbers from a $\\text{Uniform}(0,1)$ distribution, and fill a matrix of shape a $ \\times $ b (rows $\\times$ columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "62502c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27894163],\n",
       "       [0.37623856],\n",
       "       [0.44217684],\n",
       "       [0.50235654],\n",
       "       [0.47850427]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate 2 columns of X data, first column with 1's\n",
    "X1 = np.ones((5,1)); X2 = np.random.rand(5,1)\n",
    "# Slap columns together in one matrix\n",
    "X = np.c_[X1, X2]\n",
    "# Generate 2 random betas\n",
    "b = np.random.rand(2,1)\n",
    "# Project!\n",
    "linear_projection(X,b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e5d8a60",
   "metadata": {},
   "source": [
    " <br> <br> <br> <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d9c1539",
   "metadata": {},
   "source": [
    "### 2. Data generating process\n",
    "We will create data that has N observations and a \"true\" but noisy relationship between $x$ and $y$. This type of data is used in Monte Carlo simulations to test theory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cabcc341",
   "metadata": {},
   "source": [
    "#### Aside 2: Random Sampling\n",
    "- To sample from a $\\text{Uniform}(0,1)$ distribution, use `numpy.random.rand(a,b)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1d203bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03816418],\n",
       "       [0.56551672],\n",
       "       [0.65887349],\n",
       "       [0.18326635],\n",
       "       [0.79741131]])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(5,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93b72c70",
   "metadata": {},
   "source": [
    "- We can sample from other distributions. For example, we can choose `norm()` from `scipy.stats.distributions` to get standard $\\text{Normal}$ distribution.\n",
    "- To sample from a distribution, use the `scipy.stats.distributions.[DIST].rvs()` function from `scipy.stats.distributions`.\n",
    "- We renamed `scipy.stats.distributions` as  `iid` when importing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "587fb3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.54151495],\n",
       "       [1.04394466],\n",
       "       [2.10778515]])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a distribution\n",
    "DIST = iid.norm()\n",
    "# Sample from that distribution\n",
    "DIST.rvs((3,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf4c6809",
   "metadata": {},
   "source": [
    "- To create code that is reproduceable, we can \"set the randomizer seed.\" This selects and resets the specific random generation process.\n",
    "- Every time you set the seed and then resample, you will get the same \"random\" numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "59d8fb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.47143516],\n",
       "       [-1.19097569],\n",
       "       [ 1.43270697]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample some \"random\" numbers\n",
    "np.random.seed(seed=1234)\n",
    "DIST.rvs((3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b6bb29de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.47143516],\n",
       "       [-1.19097569],\n",
       "       [ 1.43270697]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try sampling again!\n",
    "np.random.seed(seed=1234)\n",
    "DIST.rvs((3,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c218523",
   "metadata": {},
   "source": [
    "- You can use this in Monte Carlo simulations by generating random data using predefined seeds. This means any time someone re-runs your code, they should get the same output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4480017",
   "metadata": {},
   "source": [
    "- <font color='#ba2121'>**Reminder:**</font> `rand()` and `rvs()` take integers (`int`'s) as inputs -- they are the shape of the output matrix.\n",
    "- In python, there are many data types, like `float` and `bool`.\n",
    "- You cannot use `float`'s in `rand()` -- see below output.\n",
    "- see the `1_fundamentals/Day 1.1 - Python Data Types.ipynb` notebook for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1d41f57c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[228], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrand(\u001b[39m5.0\u001b[39;49m,\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mmtrand.pyx:1182\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.rand\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mmtrand.pyx:425\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.random_sample\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_common.pyx:307\u001b[0m, in \u001b[0;36mnumpy.random._common.double_fill\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "np.random.rand(5.0,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "079fb53f",
   "metadata": {},
   "source": [
    "*end of Aside 2* \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f203448f",
   "metadata": {},
   "source": [
    "#### Back to generating random data...\n",
    "Make a function that generates X and Y data from $\\beta$ values we give it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "38d574a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataGenerator(beta, N, seed=1234):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - beta: A Kx1 numpy array. True beta from which to generate data.\n",
    "        - N: Number of observations the data should have\n",
    "        - seed: (optional) integer to make the random number generation repeatable.\n",
    "    Returns:\n",
    "        - X: A numpy array of random data with shape NxK\n",
    "        - y: A numpy array generated by Xbeta + e with shape Nx1\n",
    "    '''\n",
    "    # Set the seed for generating random numbers\n",
    "    np.random.seed(seed=seed)\n",
    "    # create an X vector\n",
    "    # note I instantiate iid.norm() and call method rvs() in the same step!\n",
    "    x = iid.norm().rvs((N,1))\n",
    "\n",
    "    # create a random error\n",
    "    e = iid.norm().rvs((N,1))\n",
    "    # add an intercept by horizontally stacking x with an array of ones,\n",
    "    X = np.c_[np.ones((N,1)), x]\n",
    "    # make sure beta is the right shape: Kx1\n",
    "    beta = beta.reshape((beta.shape[0],1))\n",
    "    # create y\n",
    "    y = linear_projection(X, beta) + e\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58e9a6e9",
   "metadata": {},
   "source": [
    "Test the function: give the function a true $\\beta$ vector and number of observations `N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "bef2c7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.        ,  0.47143516],\n",
       "        [ 1.        , -1.19097569],\n",
       "        [ 1.        ,  1.43270697],\n",
       "        [ 1.        , -0.3126519 ],\n",
       "        [ 1.        , -0.72058873]]),\n",
       " array([[ 2.3585981 ],\n",
       "        [ 0.66861272],\n",
       "        [ 1.79618346],\n",
       "        [ 0.70304448],\n",
       "        [-1.96327369]]))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "beta_true = np.array([1,1])\n",
    "dataGenerator(beta_true, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "094b7c17",
   "metadata": {},
   "source": [
    "Try again -- will the data be the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "ab5bcb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.        ,  0.47143516],\n",
       "        [ 1.        , -1.19097569],\n",
       "        [ 1.        ,  1.43270697],\n",
       "        [ 1.        , -0.3126519 ],\n",
       "        [ 1.        , -0.72058873]]),\n",
       " array([[ 2.3585981 ],\n",
       "        [ 0.66861272],\n",
       "        [ 1.79618346],\n",
       "        [ 0.70304448],\n",
       "        [-1.96327369]]))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "beta_true = np.array([1,1])\n",
    "dataGenerator(beta_true, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92925a79",
   "metadata": {},
   "source": [
    "**Dimension check:** How many elements are in the output from `dataGenerator()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "c0faac84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataGenerator(beta_true, 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ffcb1f6",
   "metadata": {},
   "source": [
    "**Type check:** What type of object is the output from `dataGenerator()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "03914d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataGenerator(beta_true, 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ea47456",
   "metadata": {},
   "source": [
    "Now create the data that we will use for the rest of the notebook by assigning it to the variables `X` and `y`. Because the output from `dataGenerator()` is a tuple, we can \"unpack\" the elements of the tuple directly into other variables using the following synatax:\n",
    "```python\n",
    "A = (1, 2, 3)  # this is a tuple of length 3\n",
    "a, b, c = A    # this unpacks the elements of A into new variables a,b,c\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "586ef35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is 100 x 2\n",
      "y is 100 x 1\n"
     ]
    }
   ],
   "source": [
    "# create data\n",
    "beta_true = np.array([2,8])\n",
    "N = 100\n",
    "\n",
    "X, y = dataGenerator(beta_true, N)\n",
    "\n",
    "# test shapes!\n",
    "print(f\"X is {X.shape[0]} x {X.shape[1]}\")\n",
    "print(f\"y is {y.shape[0]} x {y.shape[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c2c143c",
   "metadata": {},
   "source": [
    " <br> <br> <br> <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac5290c5",
   "metadata": {},
   "source": [
    "### 3. Minimizer function\n",
    "In order to minimize, we are going to use a minimizer function from ``scipy.optimize``. The documentation for this function can be found [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html), but the key arguments are the following:\n",
    "* ``fun``: the function to be minimized. This must be a function of only one input; if there are multiple inputs, we will \"mask\" these using lambda functions. Note that functions can be passed to other functions! Functions are objects just like other python data types, so we can pass them around using their name.\n",
    "* ``x0``: The start guess for the solution. In the case that the solution has a global minimum (as the least squares problem does) the choice will only affect computation time.\n",
    "\n",
    "Thus the final syntax is `minimize(fun = function(x), x0 = [start guess])`.\n",
    "\n",
    "The function returns an instance of the ``OptimizeResult`` class, which has several attributes. The only one we will be interested in for now is ``x``, the solution that solves the minimization.\n",
    "\n",
    "Let's set up a function that returns the object we want to minimize for OLS: the sum of squared errors. Note that the SSE is given by:\n",
    "\n",
    "$$SSE = \\sum_i (\\widehat{y}_i - y_i)^2$$\n",
    "\n",
    "Implementing this sum of squared errors in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "734b3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse(y, X, b):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - y: Numpy array Nx1\n",
    "        - X: Numpy array NxK\n",
    "        - b: Numpy array Kx1\n",
    "    '''\n",
    "    yhat = linear_projection(X, b)\n",
    "    sse = np.sum((yhat - y)**2)\n",
    "    return sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "162d133b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.00922942677013"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sse(y,X,beta_true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc7186ec",
   "metadata": {},
   "source": [
    "**Knowledge check:** we're testing the true $\\beta$ vector on the `X` and `y` we generated using `dataGenerator()`... if we're using the true $\\beta$ vector, shouldn't the error be zero? Why or why not?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb5e2782",
   "metadata": {},
   "source": [
    "To minimize this function, we need to make it a function of just one variable (the variable we want to minimize over). We can do this by masking the other inputs in a lambda function. What's a lambda function?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f31196fd",
   "metadata": {},
   "source": [
    "#### Aside 3: Passing functions and lambda functions\n",
    "- Functions in python are a type of object (like a `float`, a `tuple`, or an `array`).\n",
    "- Any object in python can be passed to a function as an argument.\n",
    "- To minimize a function in python over one variable, we need to pass a one-argument function to `scipy.optimize.minimize()`.\n",
    "- The general syntax is like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f8fe95df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 2.311471135620994e-16\n",
       "        x: [-1.075e-08 -1.075e-08]\n",
       "      nit: 2\n",
       "      jac: [-6.600e-09 -6.600e-09]\n",
       " hess_inv: [[ 7.500e-01 -2.500e-01]\n",
       "            [-2.500e-01  7.500e-01]]\n",
       "     nfev: 9\n",
       "     njev: 3"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def my_fun(a):\n",
    "    return a[0]**2 + a[1]**2\n",
    "\n",
    "minimize(my_fun, x0 = [1,1])\n",
    "# `x0` tells `minimize()` where to start searching for the minimizing value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "350e453c",
   "metadata": {},
   "source": [
    "You can see that `my_fun` is being passed as an argument to `minimize()`.\n",
    "\n",
    "What if our function has multiple inputs but we only want to minimize over one? We can use a lambda function to create new function, inside the argument. Suppose we have the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "1558b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_fun2(a, b):\n",
    "    return b*(a[0]**2 + a[1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "cefe2c0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "my_fun2() missing 1 required positional argument: 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[246], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m minimize(my_fun2, x0 \u001b[39m=\u001b[39;49m [\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\A\\miniconda3\\envs\\are212-bootcamp\\lib\\site-packages\\scipy\\optimize\\_minimize.py:691\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    689\u001b[0m     res \u001b[39m=\u001b[39m _minimize_cg(fun, x0, args, jac, callback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    690\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbfgs\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 691\u001b[0m     res \u001b[39m=\u001b[39m _minimize_bfgs(fun, x0, args, jac, callback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    692\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnewton-cg\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    693\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    694\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\A\\miniconda3\\envs\\are212-bootcamp\\lib\\site-packages\\scipy\\optimize\\_optimize.py:1362\u001b[0m, in \u001b[0;36m_minimize_bfgs\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, **unknown_options)\u001b[0m\n\u001b[0;32m   1359\u001b[0m \u001b[39mif\u001b[39;00m maxiter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1360\u001b[0m     maxiter \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(x0) \u001b[39m*\u001b[39m \u001b[39m200\u001b[39m\n\u001b[1;32m-> 1362\u001b[0m sf \u001b[39m=\u001b[39m _prepare_scalar_function(fun, x0, jac, args\u001b[39m=\u001b[39;49margs, epsilon\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m   1363\u001b[0m                               finite_diff_rel_step\u001b[39m=\u001b[39;49mfinite_diff_rel_step)\n\u001b[0;32m   1365\u001b[0m f \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39mfun\n\u001b[0;32m   1366\u001b[0m myfprime \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39mgrad\n",
      "File \u001b[1;32mc:\\Users\\A\\miniconda3\\envs\\are212-bootcamp\\lib\\site-packages\\scipy\\optimize\\_optimize.py:332\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    328\u001b[0m     bounds \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf, np\u001b[39m.\u001b[39minf)\n\u001b[0;32m    330\u001b[0m \u001b[39m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[39m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m sf \u001b[39m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0;32m    333\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[39m=\u001b[39;49mepsilon)\n\u001b[0;32m    335\u001b[0m \u001b[39mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32mc:\\Users\\A\\miniconda3\\envs\\are212-bootcamp\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx)\n\u001b[0;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun_impl \u001b[39m=\u001b[39m update_fun\n\u001b[1;32m--> 158\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[0;32m    160\u001b[0m \u001b[39m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mif\u001b[39;00m callable(grad):\n",
      "File \u001b[1;32mc:\\Users\\A\\miniconda3\\envs\\are212-bootcamp\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\A\\miniconda3\\envs\\are212-bootcamp\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[1;32mc:\\Users\\A\\miniconda3\\envs\\are212-bootcamp\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "\u001b[1;31mTypeError\u001b[0m: my_fun2() missing 1 required positional argument: 'b'"
     ]
    }
   ],
   "source": [
    "minimize(my_fun2, x0 = [1,1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "075a1c41",
   "metadata": {},
   "source": [
    "This produces an error because `minimize` can only take a function of one argument, yet `my_fun2` takes two arguments....\n",
    "\n",
    "Lambda functions just create a new \"anonymous\" function (a funtion without a name). The below lambda function is the same as `my_fun()` but does not have an assigned name to it.\n",
    "```python\n",
    "lambda a: a[0]**2 + a[1]**2\n",
    "```\n",
    "\n",
    "This allows us to create new functions...\n",
    "```python\n",
    "my_fun2 = lambda a: my_fun(a)\n",
    "```\n",
    "\n",
    "and newer functions that are functions of some but not all of the same arguments:\n",
    "```python\n",
    "b = 2\n",
    "my_fun3 = lambda a: my_fun2(a, b)\n",
    "```\n",
    "\n",
    "This allows us to pass `minimize()` a new function that only has one argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "273d1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 2\n",
    "my_fun3 = lambda a: my_fun2(a, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f7c9b32",
   "metadata": {},
   "source": [
    "We can put our new function directly inside the minimize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "8ae62402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 4.622942271241988e-16\n",
       "        x: [-1.075e-08 -1.075e-08]\n",
       "      nit: 2\n",
       "      jac: [-1.320e-08 -1.320e-08]\n",
       " hess_inv: [[ 6.250e-01 -3.750e-01]\n",
       "            [-3.750e-01  6.250e-01]]\n",
       "     nfev: 9\n",
       "     njev: 3"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(my_fun3, x0 = [1,1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41b04e6f",
   "metadata": {},
   "source": [
    "We can also define the lambda function direclty inside the `minimize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "f81e34f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 4.622942271241988e-16\n",
       "        x: [-1.075e-08 -1.075e-08]\n",
       "      nit: 2\n",
       "      jac: [-1.320e-08 -1.320e-08]\n",
       " hess_inv: [[ 6.250e-01 -3.750e-01]\n",
       "            [-3.750e-01  6.250e-01]]\n",
       "     nfev: 9\n",
       "     njev: 3"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(lambda a: my_fun2(a, b), x0 = [1,1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3978768f",
   "metadata": {},
   "source": [
    "*end of Aside 3* \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e7f379f",
   "metadata": {},
   "source": [
    "**Back to minimizing our sum of squared errors...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ab0b5c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c1fb1be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 95.1005620152201\n",
       "        x: [ 1.947e+00  8.081e+00]\n",
       "      nit: 6\n",
       "      jac: [-9.537e-07 -1.907e-06]\n",
       " hess_inv: [[ 5.006e-03 -1.771e-04]\n",
       "            [-1.771e-04  5.043e-03]]\n",
       "     nfev: 27\n",
       "     njev: 9"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember the syntax: \n",
    "# minimize(fun = function(x), x0 = [start guess])\n",
    "# the lambda function allows sse to be a function of only x, the other inputs\n",
    "# come from the variables X and y we already defined.\n",
    "minimize(lambda x: sse(y, X, x), x0 = [0,0])\n",
    "# as expected, we get an intercept of around 2 and a slope around 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0ee0f38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 9881.846756005334\n",
       "        x: [ 1.998e+00  7.994e+00]\n",
       "      nit: 7\n",
       "      jac: [ 0.000e+00  0.000e+00]\n",
       " hess_inv: [[ 5.012e-05 -2.255e-06]\n",
       "            [-2.255e-06  5.045e-05]]\n",
       "     nfev: 48\n",
       "     njev: 16"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do it again with a higher N, letting the LLN work for us!\n",
    "X, y = dataGenerator(beta_true, 10000)\n",
    "minimize(lambda x: sse(y, X, x), x0 = [0,0])\n",
    "# now it's even more accurate!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e336fec4",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16164ed8",
   "metadata": {},
   "source": [
    "## Creating an OLS class\n",
    "Now let's take our functions and organize them into an OLS class. What is a python class? \n",
    "\n",
    "python classes are objects that have attributes and methods. These objects are abstract in the sense that a class in the abstract is used to \"instantiate\" specific \"instances\" of the class. For example, here we will create a class called OLS, and will create several instantiations of specific OLS models. That is, the OLS class might have attributes like X and y data, but an OLS class _instance_ will have _specific_ data for X and y.\n",
    "\n",
    "To define a class, we use ``class ClassName:``, followed by indented lines (convention is to name classes with upper case).\n",
    "\n",
    "Classes have 3 components:\n",
    "* **The constructor** this is a method (think: function) that creates an _instance_ of the class. This component must be present in all classes, as it is the engine that creates the object. It looks like ``def __init__(self):``\n",
    "* **Attributes** These are attributes that all instances of a class have. They can be anything, or not exist at all. For example, consider a Student class that stores information for a student database. In that case, we might want students to have attributes like a student ID, gender, age, etc. These can vary by instance of the Student class, but all instances of ``Student`` have one. If we imagine an alternative class called `Alien()`, attributes might include home planet and and number of limbs.\n",
    "* **Methods** Methods are functions that belong to a class. They may return output or not. For example, we may want to write a method that calculates a student's GPA given a series of numeric grades. In the case of OLS, out main method will be to estimate beta.\n",
    "\n",
    "The simplest class is made up of only a constructor, and the simplest constructor doesn't do anything except create an instance of the class. This looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b67c8ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OLS:\n",
    "    # constructor\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecee924",
   "metadata": {},
   "source": [
    "Now to create an _instance_ of the class, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "88466c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.OLS at 0x1c12689ea30>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myOLSmodel = OLS()\n",
    "myOLSmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c716fa5",
   "metadata": {},
   "source": [
    "However, this OLS model is not very interesting. For example, we might think that each OLS model might have data associated with it. Therefore, we can require that the instantiation receive X and y data, and create attributes from these data. Attributes \"belong\" to the class, and can be accessed with the syntax ``classInstance.attribute``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1d33b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OLS:\n",
    "    # constructor\n",
    "    def __init__(self, X, y):\n",
    "        # define attributes\n",
    "        self.X = X\n",
    "        self.y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6fce7d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.ones((5,1))\n",
    "x_test = np.ones((5,2))*3\n",
    "\n",
    "myOLS = OLS(x_test, y_test)\n",
    "myOLS.y # note that within OLS(), the argument is called y!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac3d98",
   "metadata": {},
   "source": [
    "Now that our OLS class has attributes, we want it to calculate something! So we are going to give it **methods**, which are functions that belong to a class. For example, let's add our linear_projection function to this class as a method. In order to do this, we define the function within the class body, and we add another special argument to the method ``self``. By adding self as an argument, we will have access to all the attributes and methods that the class contains! For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4375cd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OLS:\n",
    "    # constructor\n",
    "    def __init__(self, X, y):\n",
    "        # define attributes\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    # ------- methods --------\n",
    "    # linear projection\n",
    "    def linear_projection(self, b):\n",
    "        b = b.reshape((len(b),1))\n",
    "        return self.X@b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8094ac01",
   "metadata": {},
   "source": [
    "What do you notice is different about linear_projection the method?\n",
    "1. It has ``self`` as an argument. This allows it to know about attributes and even other methods contained in the class.\n",
    "2. I took away X as an argument, instead calling ``self.X`` in the method body. How can this be?? Since X is now an _attribute_ of the class, and the method has the ``self`` argument, ``self.X`` is saying \"grab the X that you defined as the class attribute\". This way we don't have to constantly be entering our data into all the function calls, because our OLS instance is storing it for us! Here's how we would use this method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "420cc3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.47143516],\n",
       "       [0.80902431],\n",
       "       [3.43270697],\n",
       "       ...,\n",
       "       [0.94753931],\n",
       "       [1.5023069 ],\n",
       "       [1.7439938 ]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myOLS = OLS(X, y)\n",
    "\n",
    "# method call with instanceName.method. Only input is b; we don't have to pass \"self\" to the method.\n",
    "myOLS.linear_projection(np.array([2,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd83144b",
   "metadata": {},
   "source": [
    "Now let's add our other methods. What other changes do you notice to these expressions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3a6f09f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class OLS:\n",
    "    # constructor\n",
    "    def __init__(self, X, y):\n",
    "        # define attributes\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    # ------- methods --------\n",
    "    # linear projection\n",
    "    def linear_projection(self, b):\n",
    "        b = b.reshape((len(b),1))\n",
    "        return self.X@b\n",
    "    \n",
    "    # SSE\n",
    "    def sse(self, b): # used to be a f'm of y, X, and b. Now only need b!\n",
    "        yhat = self.linear_projection(b)\n",
    "        sse = np.sum((yhat - self.y)**2)\n",
    "        return sse\n",
    "    \n",
    "    # minimize the SSE\n",
    "    def estimate(self, x0 = [0,0]):\n",
    "        # default initial guess of [0,0]\n",
    "        sol = minimize(self.sse, x0 = x0)\n",
    "        return sol.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9a8811",
   "metadata": {},
   "source": [
    "What happened to the arguments in minimize?? Here something cool happens, and it actually starts with the `sse` function. Now that `X` and `y` are attributes of the class, the `self.sse()` method knows what they are thanks to the `self` argument that is implicitly passed into it! Therefore we can call `sse()` as only a function of one argument: `b`. Here's proof:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ce1c9426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494408.17393946426"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate model\n",
    "model1 = OLS(X, y)\n",
    "\n",
    "# call sse method\n",
    "model1.sse(np.array([2,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a09912",
   "metadata": {},
   "source": [
    "Now that `OLS.sse()` is only a function of one argument, we can omit the arguments altogether in the minimze function, and call it just by its name, `self.sse`. This is an example of passing a function to another function; when there is only 1 argument, we only need its name. And because of the class structure, it already knows that the single argument is what it is minimizing over! Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b542b655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.99794054, 7.99362178])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call the solve_OLS() method\n",
    "model1.estimate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6fc7ce6",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e763bf0",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Try these out for yourself:\n",
    "1. After estimating $\\widehat{\\beta}$, add it as an attribute of the OLS class.\n",
    "2. Estimate the White-robust SEs. There are a few ways to do this; which do you prefer? Why?\n",
    "     - Estimate and return them with beta in a tuple\n",
    "     - Estimate them with beta and add as an attribute\n",
    "     - Write a method that calculates and returns them upon request (nice code will avoid re-estimating the betas each time you do this. How can this be avoided?)\n",
    "3. Rewrite the estimation in terms of matrix algebra instead of a minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2b0f765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OLS:\n",
    "    # constructor\n",
    "    def __init__(self, X, y):\n",
    "        # define attributes\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    # ------- methods --------\n",
    "    # linear projection\n",
    "    def linear_projection(self, b):\n",
    "        b = b.reshape((len(b),1))\n",
    "        return self.X@b\n",
    "    # SSE\n",
    "    def sse(self, b): # used to be a f'm of y, X, and b. Now only need b!\n",
    "        yhat = self.linear_projection(b)\n",
    "        sse = np.sum((yhat - self.y)**2)\n",
    "        return sse\n",
    "    # minimize the SSE\n",
    "    def estimate(self, x0 = [0,0]):\n",
    "        # default initial guess of [0,0]\n",
    "        sol = minimize(self.sse, x0 = x0)\n",
    "        # Exercise 1\n",
    "        self.beta = sol.x\n",
    "        \n",
    "        return sol.x\n",
    "        \n",
    "        \n",
    "    # Exercise 2\n",
    "    def whiteSEs(self):\n",
    "        if not hasattr(self, 'beta'):\n",
    "            self.estimate()\n",
    "        e = self.y - self.linear_projection(self.beta)\n",
    "        emat = np.diag((e**2).squeeze()) # squeeze to make 1D for np.diag() alt: e**2 * np.eye(length(e))\n",
    "        bread = np.linalg.inv(self.X.T@self.X)\n",
    "        whiteSE = bread@(self.X.T@emat@self.X)@bread\n",
    "        return(whiteSE)\n",
    "    \n",
    "     # Exercise 3\n",
    "    def estimate_matAlg(self):\n",
    "        return np.linalg.inv(self.X.T@self.X)@self.X.T@self.y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d9354d3",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10519bd5",
   "metadata": {},
   "source": [
    "### Test our work\n",
    "I test the results against the ``statsmodels`` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "8c299e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "60797fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.99794054, 7.99362178])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate\n",
    "model = OLS(X, y)\n",
    "# run\n",
    "model.estimate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "557efccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.88373836e-05, -1.39779499e-06],\n",
       "       [-1.39779499e-06,  1.00617947e-04]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.whiteSEs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f62d7fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White SEs: \n",
      "[[0.0101291         nan]\n",
      " [       nan 0.01016232]]\n",
      "beta_hat (minimizer): \n",
      " [2.00444031 7.99636302]\n",
      "beta_hat (matrix algebra): \n",
      "[[2.00444032]\n",
      " [7.99636302]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_22568\\774340785.py:1: RuntimeWarning: invalid value encountered in sqrt\n",
      "  print(f\"White SEs: \\n{model.whiteSEs()**(1/2)}\")\n"
     ]
    }
   ],
   "source": [
    "print(f\"White SEs: \\n{model.whiteSEs()**(1/2)}\")\n",
    "print(f\"beta_hat (minimizer): \\n {model.beta}\")\n",
    "print(f\"beta_hat (matrix algebra): \\n{model.estimate_matAlg()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d9195f0",
   "metadata": {},
   "source": [
    "Note that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15c24d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_ols = sm.OLS(y, X).fit(cov_type='HC0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fa84f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels White SEs:\n",
      " [[0.01003197 0.00160577]\n",
      " [0.00160577 0.01030198]]\n",
      "statsmodels beta_hat:\n",
      "[1.99789902 8.00919577]\n"
     ]
    }
   ],
   "source": [
    "print(\"statsmodels White SEs:\\n %s\" % robust_ols.cov_params()**(1/2))\n",
    "print(\"statsmodels beta_hat:\\n%s\" % robust_ols.params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "are212-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fec3f448142397d3047dbf56c4c5d7884b4174a5f36e20142d2b0eafca307a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
