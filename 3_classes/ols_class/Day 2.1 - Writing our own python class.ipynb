{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07a62193",
   "metadata": {},
   "source": [
    "# Day 2.1: Making our own OLS class\n",
    "\n",
    "Today we are going to write our own OLS class, plus some helper functions that generate random data. Specifically our goals are going to be:\n",
    "- Make a linear projection function with b0, b1, x as input, y as output\n",
    "- Write a data generating function\n",
    "- Give a brief explanation of the scipy.optimize.minimize function\n",
    "- Minimize the squared errors to estimate b0 and b1\n",
    "- Create a class that implements the same minimization, \n",
    "  that takes data in instantiation, and has an 'estimate' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13acb569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for later\n",
    "import numpy as np\n",
    "from scipy.stats import distributions as iid\n",
    "from scipy.stats import rv_continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eba71a",
   "metadata": {},
   "source": [
    "## Building blocks of an OLS class\n",
    "Now, before defining the OLS class methods, we are going to write its methods first outside the class so that we know they are behaving properly. Our goal is to write 2 main methods:\n",
    "1. Linear projection\n",
    "    - inputs: b0, b1, x\n",
    "    - outputs: y\n",
    "2. Minimizer function: minimizes the squared distance between the linear projection and y\n",
    "\n",
    "### 1. Linear projection\n",
    "Suppose we have a vector X that is N by 2, where the first column is a column of ones, and a vector of betas: b = [b0, b1]. The projection matrix, or the matrix that predicts y, is given by $Xb$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e01991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_projection(X, b):\n",
    "    return X@b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd8c255",
   "metadata": {},
   "source": [
    "### 2. Data generating process\n",
    "We will create data that has N observations and a \"true\" but noisy relationship between $x$ and $y$. This type of data is used in Monte Carlo simulations to test theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef2c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create some data with a function that takes a true\n",
    "# value for beta, and a desired number of observ. and creates X and y\n",
    "def dataGenerator(beta, N):\n",
    "    # create an X vector\n",
    "    # note I instantiate iid.norm() and call method rvs() in the same step!\n",
    "    x = iid.norm().rvs(N)\n",
    "\n",
    "    # create a random error\n",
    "    e = iid.norm().rvs(N)\n",
    "    # add an intercept by vertically stacking x with an array of ones,\n",
    "    # then transposing\n",
    "    X = np.vstack((np.ones(N), x)).T\n",
    "    # create y\n",
    "    y = linear_projection(X, beta) + e\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "586ef35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data\n",
    "beta_true = [2,8]\n",
    "N = 100\n",
    "\n",
    "X, y = dataGenerator(beta_true, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5290c5",
   "metadata": {},
   "source": [
    "### 3. Minimizer function\n",
    "In order to minimize, we are going to use a minimizer function from ``scipy.optimize``. The documentation for this function can be found [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html), but the key arguments are the following:\n",
    "* ``fun``: the function to be minimized. This must be a function of only one input; if there are multiple inputs, we will \"mask\" these using lambda functions.\n",
    "* ``x0``: The start guess for the solution. In the case that the solution has a global minimum (as the least squares problem does) the choice will only affect computation time.\n",
    "\n",
    "Thus the final syntax is `minimize(fun = function(x), x0 = [start guess])`.\n",
    "\n",
    "The function returns an instance of the ``OptimizeResult`` class, which has several attributes. The only one we will be interested in for now is ``x``, the solution that solves the minimization.\n",
    "\n",
    "Let's set up a function that returns the object we want to minimize: the sum of squared errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "734b3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse(y, X, b):\n",
    "    yhat = linear_projection(X, b)\n",
    "    sse = np.sum((yhat - y)**2)\n",
    "    return sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162d133b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104.25665958321787"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sse(y,X,beta_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e2782",
   "metadata": {},
   "source": [
    "Now we can minimize this function, making it a function of just one variable by masking the other inputs in a lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab0b5c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1fb1be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 103.78375308117808\n",
       " hess_inv: array([[0.00508468, 0.00065599],\n",
       "       [0.00065599, 0.00508199]])\n",
       "      jac: array([0.00000000e+00, 2.86102295e-06])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 27\n",
       "      nit: 6\n",
       "     njev: 9\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([1.93331405, 7.97253081])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember the syntax: \n",
    "# minimize(fun = function(x), x0 = [start guess])\n",
    "# the lambda function allows sse to be a function of only x, the other inputs\n",
    "# come from the variables X and y we already defined.\n",
    "minimize(lambda x: sse(y, X, x), x0 = [0,0])\n",
    "# as expected, we get an intercept of around 2 and a slope around 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ee0f38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 10059.48078953753\n",
       " hess_inv: array([[8.43026489e-05, 1.81437281e-06],\n",
       "       [1.81437281e-06, 2.53799953e-06]])\n",
       "      jac: array([0.00012207, 0.        ])\n",
       "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
       "     nfev: 45\n",
       "      nit: 7\n",
       "     njev: 15\n",
       "   status: 2\n",
       "  success: False\n",
       "        x: array([2.01184193, 8.01159676])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do it again with a higher N, letting the LLN work for us!\n",
    "X, y = dataGenerator(beta_true, 10000)\n",
    "minimize(lambda x: sse(y, X, x), x0 = [0,0])\n",
    "# now it's even more accurate!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16164ed8",
   "metadata": {},
   "source": [
    "## Creating an OLS class\n",
    "Now let's take our functions and organize them into an OLS class. This class will have attributes X and y, and methods defined by the functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a6f09f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OLS:\n",
    "    # constructor\n",
    "    def __init__(self, X, y):\n",
    "        # define attributes\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    ##### methods\n",
    "    # linear projection\n",
    "    def linear_projection(self, b):\n",
    "        return self.X@b\n",
    "    # SSE\n",
    "    def sse(self, b):\n",
    "        yhat = linear_projection(self.X, b)\n",
    "        sse = np.sum((yhat - self.y)**2)\n",
    "        return sse\n",
    "    # minimize the SSE\n",
    "    def estimate(self, x0 = [0,0]):\n",
    "        # default initial guess of [0,0]\n",
    "        sol = minimize(self.sse, x0 = x0)\n",
    "        return sol.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9a8811",
   "metadata": {},
   "source": [
    "What happened to the arguments in minimize?? Here something cool happens, and it actually starts with the `sse` function. Now that `X` and `y` are attributes of the class, the `self.sse()` method knows what they are thanks to the `self` argument that is implicitly passed into it! Therefore we can call `sse()` as only a function of one argument: `b`. Here's proof:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce1c9426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496511.1877916631"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate model\n",
    "model1 = OLS(X, y)\n",
    "\n",
    "# call sse method\n",
    "model1.sse([2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a09912",
   "metadata": {},
   "source": [
    "Now that `OLS.sse()` is only a function of one argument, we can omit the arguments altogether in the minimze function, and call it just by its name, `self.sse`. It already knows that the single argument is what it is minimizing over! Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b542b655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.01184193, 8.01159676])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call the solve_OLS() method\n",
    "model1.estimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e763bf0",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Try these out for yourself:\n",
    "1. After estimating $\\widehat{\\beta}$, add it as an attribute of the OLS class.\n",
    "2. Estimate the White-robust SEs. There are a few ways to do this; which do you prefer? Why?\n",
    "     - Estimate and return them with beta in a tuple\n",
    "     - Estimate them with beta and add as an attribute\n",
    "     - Write a method that calculates and returns them upon request (nice code will avoid re-estimating the betas each time you do this. How can this be avoided?)\n",
    "3. Rewrite the estimation in terms of matrix algebra instead of a minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b0f765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OLS:\n",
    "    # constructor\n",
    "    def __init__(self, X, y):\n",
    "        # define attributes\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    ##### methods\n",
    "    # linear projection\n",
    "    def linear_projection(self, b):\n",
    "        return self.X@b\n",
    "    # SSE\n",
    "    def sse(self, b):\n",
    "        yhat = linear_projection(self.X, b)\n",
    "        sse = np.sum((yhat - self.y)**2)\n",
    "        return sse\n",
    "    # minimize the SSE\n",
    "    def estimate(self, x0 = [0,0]):\n",
    "        # default initial guess of [0,0]\n",
    "        sol = minimize(self.sse, x0 = x0)\n",
    "        # Exercise 1\n",
    "        self.beta = sol.x\n",
    "    # Exercise 2\n",
    "    def whiteSEs(self):\n",
    "        if not hasattr(self, 'beta'):\n",
    "            self.estimate()\n",
    "        emat = np.diag((self.y - self.linear_projection(self.beta))**2)\n",
    "        bread = np.linalg.inv(X.T@X)\n",
    "        whiteSE = bread@(X.T@emat@X)@bread\n",
    "        return(whiteSE)\n",
    "     # Exercise 3\n",
    "    def estimate_matAlg(self):\n",
    "        return np.linalg.inv(X.T@X)@X.T@y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10519bd5",
   "metadata": {},
   "source": [
    "### Test our work\n",
    "I test the results against the ``statsmodels`` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c299e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60797fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate\n",
    "model = OLS(X, y)\n",
    "# run\n",
    "model.estimate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f62d7fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White SEs: \n",
      "[[0.01003138 0.00138059]\n",
      " [0.00138059 0.01011787]]\n",
      "beta_hat (minimizer): \n",
      " [2.01184193 8.01159676]\n",
      "beta_hat (matrix algebra): \n",
      "[2.01184193 8.01159676]\n"
     ]
    }
   ],
   "source": [
    "print(\"White SEs: \\n%s\" % model.whiteSEs()**(1/2))\n",
    "print(\"beta_hat (minimizer): \\n %s\" % model.beta)\n",
    "print(\"beta_hat (matrix algebra): \\n%s\" % model.estimate_matAlg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15c24d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_ols = sm.OLS(y, X).fit(cov_type='HC1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fa84f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels robust SEs:\n",
      " [[0.01003238 0.00138073]\n",
      " [0.00138073 0.01011888]]\n",
      "statsmodels beta_hat:\n",
      "[2.01184193 8.01159676]\n"
     ]
    }
   ],
   "source": [
    "print(\"statsmodels robust SEs:\\n %s\" % robust_ols.cov_params()**(1/2))\n",
    "print(\"statsmodels beta_hat:\\n%s\" % robust_ols.params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
